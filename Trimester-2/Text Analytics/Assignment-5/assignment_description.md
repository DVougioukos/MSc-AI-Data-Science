**Repeat Exercise 2 of Part 5 (sentiment classifier), by fine-tuning a pre-trained BERT**

**model.1 Tune the hyper-parameters (e.g., sizes of any task-specific layers on top of BERT,**

**number of BERT encoder blocks to keep frozen) on the development subset of your dataset.**

**Monitor the performance of your models on the development subset during training to decide**

**how many epochs to use. If the texts of your experiments exceed BERTâ€™s maximum length**

**limit, you may want to truncate them at the maximum allowed length of BERT or use a**

**BERT-like model that can handle longer texts (e.g., Longformer).2 Include experimental**

**results of a baseline majority classifier, as well as experimental results of your best classifiers**

**from exercise 15 of Part 2, exercise 9 of Part 3, exercise 1 of Part 4, exercise 2 of Part 5, now**

**treated as additional baselines. Otherwise, the contents of your report should be as in exercise**

**2 of Part 5, but now with information and results for the experiments of this exercise. You**

**may optionally include (for extra bonus) indicative experimental results on a small subset of**

**the test set (e.g., 10 test examples) obtained by prompting an LLM (e.g., Chat-GPT), using**

**appropriate instructions and possibly including few-shot examples (demonstrators).3**

